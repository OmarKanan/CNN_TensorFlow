{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import functools\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import data_reader\n",
    "from batch_norm import Batch_Normalizer\n",
    "from model import CNN_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File parameters\n",
    "DATA_DIR = 'data'\n",
    "TRAIN_IMAGES = 'data_train.bin'\n",
    "TRAIN_TEMPLATES = 'fv_train.bin'\n",
    "VALID_IMAGES = 'data_valid.bin'\n",
    "VALID_TEMPLATES = 'fv_valid.bin'\n",
    "TEST_IMAGES = 'data_test.bin'\n",
    "\n",
    "# Data parameters\n",
    "NUM_TRAIN_IMAGES = 100000\n",
    "NUM_VALID_IMAGES = 10000\n",
    "NUM_TEST_IMAGES = 10000\n",
    "IMAGE_DIM = 48\n",
    "TEMPLATE_DIM = 128\n",
    "\n",
    "# Batch parameters\n",
    "BATCH_SIZE = 100\n",
    "VALID_BATCH_SIZE = 1000\n",
    "\n",
    "# Batch norm parameters\n",
    "EMA_DECAY = 0.99\n",
    "BN_EPSILON = 0.01\n",
    "\n",
    "# Log parameters\n",
    "LOG_DIR = 'logs'\n",
    "SAVE_DIR = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run1():\n",
    "    model = CNN_Model('config3', DATA_DIR, LOG_DIR, SAVE_DIR, IMAGE_DIM, TEMPLATE_DIM, TRAIN_IMAGES, TRAIN_TEMPLATES, \n",
    "                      VALID_IMAGES, VALID_TEMPLATES, NUM_TRAIN_IMAGES, NUM_VALID_IMAGES, EMA_DECAY, BN_EPSILON)\n",
    "\n",
    "    model.add_conv('conv_1_1', ksize=[3, 3, 1, 10])\n",
    "    model.add_batch_norm('bn_1_1')\n",
    "    model.add_conv('conv_1_2', ksize=[3, 3, 10, 10])\n",
    "    model.add_pool('max_pool_1', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "    model.add_batch_norm('bn_1_2')\n",
    "\n",
    "    model.add_conv('conv_2_1', ksize=[3, 3, 10, 20])\n",
    "    model.add_batch_norm('bn_2_1')\n",
    "    model.add_conv('conv_2_2', ksize=[3, 3, 20, 20])\n",
    "    model.add_pool('max_pool_2', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "    model.add_batch_norm('bn_2_2')\n",
    "\n",
    "    model.add_conv('conv_3_1', ksize=[3, 3, 20, 40])\n",
    "    model.add_batch_norm('bn_3_1')\n",
    "    model.add_conv('conv_3_2', ksize=[3, 3, 40, 40])\n",
    "    model.add_pool('avg_pool_3', 'avg', ksize=[1, 6, 6, 1], stride=[1, 6, 6, 1])\n",
    "    model.add_batch_norm('bn_3_2')\n",
    "\n",
    "    model.add_fully_connected('fc', size=128)\n",
    "\n",
    "    model.add_mse_loss('mse_loss')\n",
    "    model.add_adam_optimizer('optimizer', init_learning_rate=0.01, decay=False)\n",
    "    model.add_summaries('summaries')\n",
    "\n",
    "    \n",
    "    utils.add_description(\"\"\"\n",
    "    model = CNN_Model('config3', DATA_DIR, LOG_DIR, SAVE_DIR, IMAGE_DIM, TEMPLATE_DIM, TRAIN_IMAGES, TRAIN_TEMPLATES, \n",
    "                      VALID_IMAGES, VALID_TEMPLATES, NUM_TRAIN_IMAGES, NUM_VALID_IMAGES, EMA_DECAY, BN_EPSILON)\n",
    "\n",
    "    model.add_conv('conv_1_1', ksize=[3, 3, 1, 10])\n",
    "    model.add_batch_norm('bn_1_1')\n",
    "    model.add_conv('conv_1_2', ksize=[3, 3, 10, 10])\n",
    "    model.add_pool('max_pool_1', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "    model.add_batch_norm('bn_1_2')\n",
    "\n",
    "    model.add_conv('conv_2_1', ksize=[3, 3, 10, 20])\n",
    "    model.add_batch_norm('bn_2_1')\n",
    "    model.add_conv('conv_2_2', ksize=[3, 3, 20, 20])\n",
    "    model.add_pool('max_pool_2', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "    model.add_batch_norm('bn_2_2')\n",
    "\n",
    "    model.add_conv('conv_3_1', ksize=[3, 3, 20, 40])\n",
    "    model.add_batch_norm('bn_3_1')\n",
    "    model.add_conv('conv_3_2', ksize=[3, 3, 40, 40])\n",
    "    model.add_pool('avg_pool_3', 'avg', ksize=[1, 6, 6, 1], stride=[1, 6, 6, 1])\n",
    "    model.add_batch_norm('bn_3_2')\n",
    "\n",
    "    model.add_fully_connected('fc', size=128)\n",
    "\n",
    "    model.add_mse_loss('mse_loss')\n",
    "    model.add_adam_optimizer('optimizer', init_learning_rate=0.01, decay=False)\n",
    "    model.add_summaries('summaries')\n",
    "\n",
    "    total parameters = %d\n",
    "    \"\"\"  % (utils.num_parameters(model)), model.save_dir, model.name)\n",
    "    \n",
    "    \n",
    "    model.initialize_session(restore=True)\n",
    "    model.train(n_batches=10000, step_size=500, batch_size=100, valid_batch_size=1000, save=True)\n",
    "    model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description added at checkpoints\\config3\\config3_description.txt\n",
      "Restored session checkpoints\\config3\\model.ckpt-3500\n",
      "Batch 4000:\n",
      "--> Train loss = 0.007619\n",
      "--> Valid loss = 0.007717\n",
      "Saving session checkpoints\\config3\\model.ckpt-4000\n",
      "Batch 4500:\n",
      "--> Train loss = 0.007578\n",
      "--> Valid loss = 0.007763\n",
      "Saving session checkpoints\\config3\\model.ckpt-4500\n",
      "Batch 5000:\n",
      "--> Train loss = 0.007527\n",
      "--> Valid loss = 0.007697\n",
      "Saving session checkpoints\\config3\\model.ckpt-5000\n",
      "Batch 5500:\n",
      "--> Train loss = 0.007462\n",
      "--> Valid loss = 0.007665\n",
      "Saving session checkpoints\\config3\\model.ckpt-5500\n",
      "Batch 6000:\n",
      "--> Train loss = 0.007389\n",
      "--> Valid loss = 0.007642\n",
      "Saving session checkpoints\\config3\\model.ckpt-6000\n",
      "Batch 6500:\n",
      "--> Train loss = 0.007317\n",
      "--> Valid loss = 0.007652\n",
      "Saving session checkpoints\\config3\\model.ckpt-6500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-80809435f111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-d13326ebae72>\u001b[0m in \u001b[0;36mrun1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Documents\\IPython\\CNN_TensorFlow\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_batches, step_size, batch_size, valid_batch_size, save)\u001b[0m\n\u001b[1;32m    196\u001b[0m                                                    {self.images : image_batch,\n\u001b[1;32m    197\u001b[0m                                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtemplates\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtemplate_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                                                     self.is_train: True})\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mstep_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = CNN_Model('config6', DATA_DIR, LOG_DIR, SAVE_DIR, IMAGE_DIM, TEMPLATE_DIM, TRAIN_IMAGES, TRAIN_TEMPLATES, \n",
    "#                   VALID_IMAGES, VALID_TEMPLATES, NUM_TRAIN_IMAGES, NUM_VALID_IMAGES, EMA_DECAY, BN_EPSILON)\n",
    "\n",
    "# model.add_conv('conv_1', ksize=[3, 3, 1, 16])\n",
    "# model.add_pool('pool_1', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "# model.add_batch_norm('bn_1')\n",
    "\n",
    "# model.add_conv('conv_2', ksize=[3, 3, 16, 32])\n",
    "# model.add_pool('pool_2', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "# model.add_batch_norm('bn_2')\n",
    "\n",
    "# model.add_conv('conv_3', ksize=[3, 3, 32, 32])\n",
    "# model.add_pool('pool_3', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "# model.add_batch_norm('bn_3')\n",
    "\n",
    "# model.add_conv('conv_4', ksize=[3, 3, 32, 32])\n",
    "# model.add_pool('pool_4', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "# model.add_batch_norm('bn_4')\n",
    "\n",
    "# model.add_conv('conv_5', ksize=[3, 3, 32, 62])\n",
    "# model.add_pool('pool_5', 'max', ksize=[1, 3, 3, 1], stride=[1, 3, 3, 1])\n",
    "# model.add_batch_norm('bn_5')\n",
    "\n",
    "# model.add_fully_connected('fc', size=128)\n",
    "\n",
    "# model.add_mse_loss('mse_loss')\n",
    "# model.add_adam_optimizer('optimizer', init_learning_rate=0.0005, decay=False)\n",
    "# model.add_summaries('summaries')\n",
    "\n",
    "\n",
    "# test_images, _ = data_reader.load_data(DATA_DIR, TEST_IMAGES, VALID_TEMPLATES, NUM_TEST_IMAGES, IMAGE_DIM, TEMPLATE_DIM)\n",
    "# predictions = model.predict(test_images, batch_size=1000)\n",
    "\n",
    "# f = open('data/template_pred_config6.bin', 'wb')\n",
    "# for i in range(NUM_TEST_IMAGES):\n",
    "#     f.write(predictions[i, :])\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
