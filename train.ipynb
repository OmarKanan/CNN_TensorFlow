{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import functools\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import data_reader\n",
    "from batch_norm import Batch_Normalizer\n",
    "from model import CNN_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File parameters\n",
    "DATA_DIR = 'data'\n",
    "TRAIN_IMAGES = 'data_train.bin'\n",
    "TRAIN_TEMPLATES = 'fv_train.bin'\n",
    "VALID_IMAGES = 'data_valid.bin'\n",
    "VALID_TEMPLATES = 'fv_valid.bin'\n",
    "TEST_IMAGES = 'data_test.bin'\n",
    "\n",
    "# Data parameters\n",
    "NUM_TRAIN_IMAGES = 100000\n",
    "NUM_VALID_IMAGES = 10000\n",
    "NUM_TEST_IMAGES = 10000\n",
    "IMAGE_DIM = 48\n",
    "TEMPLATE_DIM = 128\n",
    "\n",
    "# Reader parameters\n",
    "BATCH_SIZE = 100\n",
    "VALID_BATCH_SIZE = 1000\n",
    "NUM_READ_THREADS = 1\n",
    "MIN_AFTER_DEQUEUE = 10000\n",
    "CAPACITY = 11000\n",
    "\n",
    "# Batch norm parameters\n",
    "EMA_DECAY = 0.99\n",
    "BN_EPSILON = 0.01\n",
    "\n",
    "# Log parameters\n",
    "LOG_DIR = 'logs'\n",
    "SAVE_DIR = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run5():\n",
    "\n",
    "    model = CNN_Model('config5', DATA_DIR, LOG_DIR, SAVE_DIR, IMAGE_DIM, TEMPLATE_DIM, TRAIN_IMAGES, TRAIN_TEMPLATES, \n",
    "                      VALID_IMAGES, VALID_TEMPLATES, NUM_TRAIN_IMAGES, NUM_VALID_IMAGES, EMA_DECAY, BN_EPSILON)\n",
    "\n",
    "    model.add_conv('conv_1_1', ksize=[3, 3, 1, 10])\n",
    "    model.add_batch_norm('bn_1_1')\n",
    "    model.add_conv('conv_1_2', ksize=[3, 3, 10, 10])\n",
    "    model.add_pool('max_pool_1', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "    model.add_batch_norm('bn_1_2')\n",
    "\n",
    "    model.add_conv('conv_2_1', ksize=[3, 3, 10, 20])\n",
    "    model.add_batch_norm('bn_2_1')\n",
    "    model.add_conv('conv_2_2', ksize=[3, 3, 20, 20])\n",
    "    model.add_pool('max_pool_2', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "    model.add_batch_norm('bn_2_2')\n",
    "\n",
    "    model.add_conv('conv_3_1', ksize=[3, 3, 20, 40])\n",
    "    model.add_batch_norm('bn_3_1')\n",
    "    model.add_conv('conv_3_2', ksize=[3, 3, 40, 40])\n",
    "    model.add_batch_norm('bn_3_2')\n",
    "    model.add_conv('conv_3_3', ksize=[3, 3, 40, 40])\n",
    "    model.add_pool('max_pool_3', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "    model.add_batch_norm('bn_3_3')\n",
    "\n",
    "    model.add_pool('avg_pool', 'avg', ksize=[1, 6, 6, 1], stride=[1, 6, 6, 1])\n",
    "    model.add_fully_connected('fc', size=128)\n",
    "\n",
    "    model.add_mse_loss('mse_loss')\n",
    "    model.add_adam_optimizer('optimizer', init_learning_rate=0.001, decay=False)\n",
    "    model.add_summaries('summaries')\n",
    "\n",
    "    \n",
    "    utils.add_description(\"\"\"\n",
    "model = CNN_Model('config5', DATA_DIR, LOG_DIR, SAVE_DIR, IMAGE_DIM, TEMPLATE_DIM, TRAIN_IMAGES, TRAIN_TEMPLATES, \n",
    "                  VALID_IMAGES, VALID_TEMPLATES, NUM_TRAIN_IMAGES, NUM_VALID_IMAGES, EMA_DECAY, BN_EPSILON)\n",
    "\n",
    "model.add_conv('conv_1_1', ksize=[3, 3, 1, 10])\n",
    "model.add_batch_norm('bn_1_1')\n",
    "model.add_conv('conv_1_2', ksize=[3, 3, 10, 10])\n",
    "model.add_pool('max_pool_1', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "model.add_batch_norm('bn_1_2')\n",
    "\n",
    "model.add_conv('conv_2_1', ksize=[3, 3, 10, 20])\n",
    "model.add_batch_norm('bn_2_1')\n",
    "model.add_conv('conv_2_2', ksize=[3, 3, 20, 20])\n",
    "model.add_pool('max_pool_2', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "model.add_batch_norm('bn_2_2')\n",
    "\n",
    "model.add_conv('conv_3_1', ksize=[3, 3, 20, 40])\n",
    "model.add_batch_norm('bn_3_1')\n",
    "model.add_conv('conv_3_2', ksize=[3, 3, 40, 40])\n",
    "model.add_batch_norm('bn_3_2')\n",
    "model.add_conv('conv_3_3', ksize=[3, 3, 40, 40])\n",
    "model.add_pool('max_pool_3', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "model.add_batch_norm('bn_3_3')\n",
    "\n",
    "model.add_pool('avg_pool', 'avg', ksize=[1, 6, 6, 1], stride=[1, 6, 6, 1])\n",
    "model.add_fully_connected('fc', size=128)\n",
    "\n",
    "model.add_mse_loss('mse_loss')\n",
    "model.add_adam_optimizer('optimizer', init_learning_rate=0.001, decay=False)\n",
    "model.add_summaries('summaries')    \n",
    "    \n",
    "total parameters = %d\n",
    "\"\"\"  % (utils.num_parameters(model)), model.save_dir, model.name)\n",
    "    \n",
    "    \n",
    "    model.initialize_session(restore=True)\n",
    "    model.train(n_batches=5000, step_size=500, batch_size=100, valid_batch_size=1000, save=True)\n",
    "    model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description added at checkpoints\\config5\\config5_description.txt\n",
      "Restored session checkpoints\\config5\\model.ckpt-5500\n",
      "Batch 6000:\n",
      "--> Train loss = 0.006361\n",
      "--> Valid loss = 0.007049\n",
      "Saving session checkpoints\\config5\\model.ckpt-6000\n",
      "Batch 6500:\n",
      "--> Train loss = 0.006340\n",
      "--> Valid loss = 0.007016\n",
      "Saving session checkpoints\\config5\\model.ckpt-6500\n",
      "Batch 7000:\n",
      "--> Train loss = 0.006340\n",
      "--> Valid loss = 0.007005\n",
      "Saving session checkpoints\\config5\\model.ckpt-7000\n",
      "Batch 7500:\n",
      "--> Train loss = 0.006335\n",
      "--> Valid loss = 0.007017\n",
      "Saving session checkpoints\\config5\\model.ckpt-7500\n",
      "Batch 8000:\n",
      "--> Train loss = 0.006333\n",
      "--> Valid loss = 0.007014\n",
      "Saving session checkpoints\\config5\\model.ckpt-8000\n",
      "Batch 8500:\n",
      "--> Train loss = 0.006329\n",
      "--> Valid loss = 0.007049\n",
      "Saving session checkpoints\\config5\\model.ckpt-8500\n",
      "Batch 9000:\n",
      "--> Train loss = 0.006333\n",
      "--> Valid loss = 0.007010\n",
      "Saving session checkpoints\\config5\\model.ckpt-9000\n",
      "Batch 9500:\n",
      "--> Train loss = 0.006328\n",
      "--> Valid loss = 0.007018\n",
      "Saving session checkpoints\\config5\\model.ckpt-9500\n",
      "Batch 10000:\n",
      "--> Train loss = 0.006326\n",
      "--> Valid loss = 0.007025\n",
      "Saving session checkpoints\\config5\\model.ckpt-10000\n",
      "Batch 10500:\n",
      "--> Train loss = 0.006323\n",
      "--> Valid loss = 0.007029\n",
      "Saving session checkpoints\\config5\\model.ckpt-10500\n"
     ]
    }
   ],
   "source": [
    "run5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
