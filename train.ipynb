{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import functools\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import data_reader\n",
    "from batch_norm import Batch_Normalizer\n",
    "from model import CNN_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File parameters\n",
    "DATA_DIR = 'data'\n",
    "TRAIN_IMAGES = 'data_train.bin'\n",
    "TRAIN_TEMPLATES = 'fv_train.bin'\n",
    "VALID_IMAGES = 'data_valid.bin'\n",
    "VALID_TEMPLATES = 'fv_valid.bin'\n",
    "TEST_IMAGES = 'data_test.bin'\n",
    "\n",
    "# Data parameters\n",
    "NUM_TRAIN_IMAGES = 100000\n",
    "NUM_VALID_IMAGES = 10000\n",
    "NUM_TEST_IMAGES = 10000\n",
    "IMAGE_DIM = 48\n",
    "TEMPLATE_DIM = 128\n",
    "\n",
    "# Reader parameters\n",
    "BATCH_SIZE = 100\n",
    "VALID_BATCH_SIZE = 1000\n",
    "NUM_READ_THREADS = 1\n",
    "MIN_AFTER_DEQUEUE = 10000\n",
    "CAPACITY = 11000\n",
    "\n",
    "# Batch norm parameters\n",
    "EMA_DECAY = 0.99\n",
    "BN_EPSILON = 0.01\n",
    "\n",
    "# Log parameters\n",
    "LOG_DIR = 'logs'\n",
    "SAVE_DIR = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description added at checkpoints\\run1\\run1_description.txt\n",
      "Total parameters : 50098\n"
     ]
    }
   ],
   "source": [
    "model = CNN_Model('run1', DATA_DIR, LOG_DIR, SAVE_DIR, IMAGE_DIM, TEMPLATE_DIM, TRAIN_IMAGES, TRAIN_TEMPLATES, \n",
    "                  VALID_IMAGES, VALID_TEMPLATES, NUM_TRAIN_IMAGES, NUM_VALID_IMAGES, EMA_DECAY, BN_EPSILON)\n",
    "\n",
    "model.add_conv('conv_1', ksize=[3, 3, 1, 10])\n",
    "model.add_batch_norm('bn_1_1')\n",
    "model.add_conv('res_conv_1', ksize=[3, 3, 10, 10], residual=-3)\n",
    "model.add_pool('max_pool_1', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1], dropout=0)\n",
    "model.add_batch_norm('bn_1_2')\n",
    "\n",
    "model.add_conv('conv_2', ksize=[3, 3, 10, 20])\n",
    "model.add_batch_norm('bn_2_1')\n",
    "model.add_conv('res_conv_2', ksize=[3, 3, 20, 20], residual=-3)\n",
    "model.add_pool('max_pool_2', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "model.add_batch_norm('bn_2_2')\n",
    "\n",
    "model.add_conv('conv_3', ksize=[3, 3, 20, 40])\n",
    "model.add_batch_norm('bn_3_1')\n",
    "model.add_conv('res_conv_3', ksize=[3, 3, 40, 40], residual=-3)\n",
    "model.add_pool('max_pool_3', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "model.add_batch_norm('bn_3_2')\n",
    "\n",
    "model.add_pool('avg_pool', 'avg', ksize=[1, 3, 3, 1], stride=[1, 3, 3, 1])\n",
    "model.add_fully_connected('fc', size=128)\n",
    "\n",
    "model.add_mse_loss('mse_loss')\n",
    "model.add_adam_optimizer('optimizer', init_learning_rate=0.01, decay=True)\n",
    "model.add_summaries('summaries')\n",
    "\n",
    "utils.add_description(\"\"\"\n",
    "model = CNN_Model('run1', DATA_DIR, LOG_DIR, SAVE_DIR, IMAGE_DIM, TEMPLATE_DIM, TRAIN_IMAGES, TRAIN_TEMPLATES, \n",
    "                  VALID_IMAGES, VALID_TEMPLATES, NUM_TRAIN_IMAGES, NUM_VALID_IMAGES, EMA_DECAY, BN_EPSILON)\n",
    "\n",
    "model.add_conv('conv_1', ksize=[3, 3, 1, 10])\n",
    "model.add_batch_norm('bn_1_1')\n",
    "model.add_conv('res_conv_1', ksize=[3, 3, 10, 10], residual=-3)\n",
    "model.add_pool('max_pool_1', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1], dropout=0)\n",
    "model.add_batch_norm('bn_1_2')\n",
    "\n",
    "model.add_conv('conv_2', ksize=[3, 3, 10, 20])\n",
    "model.add_batch_norm('bn_2_1')\n",
    "model.add_conv('res_conv_2', ksize=[3, 3, 20, 20], residual=-3)\n",
    "model.add_pool('max_pool_2', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "model.add_batch_norm('bn_2_2')\n",
    "\n",
    "model.add_conv('conv_3', ksize=[3, 3, 20, 40])\n",
    "model.add_batch_norm('bn_3_1')\n",
    "model.add_conv('res_conv_3', ksize=[3, 3, 40, 40], residual=-3)\n",
    "model.add_pool('max_pool_3', 'max', ksize=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "model.add_batch_norm('bn_3_2')\n",
    "\n",
    "model.add_pool('avg_pool', 'avg', ksize=[1, 3, 3, 1], stride=[1, 3, 3, 1])\n",
    "model.add_fully_connected('fc', size=128)\n",
    "\n",
    "model.add_mse_loss('mse_loss')\n",
    "model.add_adam_optimizer('optimizer', init_learning_rate=0.01, decay=True)\n",
    "model.add_summaries('summaries')\n",
    "\n",
    "total parameters = %d\n",
    "\"\"\" % utils.num_parameters(model), model.save_dir, model.name)\n",
    "print('Total parameters : %d' % utils.num_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200:\n",
      "--> Train loss = 0.059995\n",
      "--> Valid loss = 0.171760\n",
      "Saving session checkpoints\\run1\\model.ckpt-200\n",
      "Batch 400:\n",
      "--> Train loss = 0.042756\n",
      "--> Valid loss = 0.073106\n",
      "Saving session checkpoints\\run1\\model.ckpt-400\n",
      "Batch 600:\n",
      "--> Train loss = 0.039257\n",
      "--> Valid loss = 0.063488\n",
      "Saving session checkpoints\\run1\\model.ckpt-600\n",
      "Batch 800:\n",
      "--> Train loss = 0.037264\n",
      "--> Valid loss = 0.062447\n",
      "Saving session checkpoints\\run1\\model.ckpt-800\n",
      "Batch 1000:\n",
      "--> Train loss = 0.036172\n",
      "--> Valid loss = 0.056223\n",
      "Saving session checkpoints\\run1\\model.ckpt-1000\n",
      "Batch 1200:\n",
      "--> Train loss = 0.035081\n",
      "--> Valid loss = 0.057141\n",
      "Saving session checkpoints\\run1\\model.ckpt-1200\n",
      "Batch 1400:\n",
      "--> Train loss = 0.034274\n",
      "--> Valid loss = 0.051856\n",
      "Saving session checkpoints\\run1\\model.ckpt-1400\n",
      "Batch 1600:\n",
      "--> Train loss = 0.033707\n",
      "--> Valid loss = 0.052485\n",
      "Saving session checkpoints\\run1\\model.ckpt-1600\n",
      "Batch 1800:\n",
      "--> Train loss = 0.033230\n",
      "--> Valid loss = 0.055147\n",
      "Saving session checkpoints\\run1\\model.ckpt-1800\n",
      "Batch 2000:\n",
      "--> Train loss = 0.032771\n",
      "--> Valid loss = 0.051589\n",
      "Saving session checkpoints\\run1\\model.ckpt-2000\n",
      "Batch 2200:\n",
      "--> Train loss = 0.032342\n",
      "--> Valid loss = 0.054202\n",
      "Saving session checkpoints\\run1\\model.ckpt-2200\n",
      "Batch 2400:\n",
      "--> Train loss = 0.031750\n",
      "--> Valid loss = 0.049921\n",
      "Saving session checkpoints\\run1\\model.ckpt-2400\n",
      "Batch 2600:\n",
      "--> Train loss = 0.031286\n",
      "--> Valid loss = 0.048953\n",
      "Saving session checkpoints\\run1\\model.ckpt-2600\n",
      "Batch 2800:\n",
      "--> Train loss = 0.031551\n",
      "--> Valid loss = 0.049933\n",
      "Saving session checkpoints\\run1\\model.ckpt-2800\n",
      "Batch 3000:\n",
      "--> Train loss = 0.031115\n",
      "--> Valid loss = 0.050654\n",
      "Saving session checkpoints\\run1\\model.ckpt-3000\n",
      "Batch 3200:\n",
      "--> Train loss = 0.030546\n",
      "--> Valid loss = 0.048621\n",
      "Saving session checkpoints\\run1\\model.ckpt-3200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-abbf675cd8ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Omar\\Documents\\IPython\\CNN_TensorFlow\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_batches, step_size, batch_size, valid_batch_size, save)\u001b[0m\n\u001b[1;32m    192\u001b[0m                                                    {self.images : image_batch,\n\u001b[1;32m    193\u001b[0m                                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtemplates\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtemplate_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                                                     self.is_train: True})\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mstep_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Omar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.initialize_session(restore=True)\n",
    "model.train(n_batches=10000, step_size=200, batch_size=100, valid_batch_size=1000, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
